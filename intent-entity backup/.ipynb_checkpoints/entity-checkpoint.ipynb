{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ee2174a-1409-4d4d-bbc6-4c58e8b880a8",
   "metadata": {},
   "source": [
    "Step 1- Create a text file of train and dev data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99347ad4-f984-4f64-b8bc-09c365ee53ed",
   "metadata": {},
   "source": [
    "step 2 - from ner annotator create custom tags and make annotated data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9d85bd1-41eb-4935-9b55-9115f27f4dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open('annotations_train.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "a566dd06-05a7-4c6d-8445-96db59a61461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def handle_possessive_names(text):\n",
    "    return text.replace(\"'s\", \"\")\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    text = handle_possessive_names(text)\n",
    "    text = text.lower()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "95b229ef-b417-44a5-aaf8-513611f7e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data['annotations'] \n",
    "train_data = [(preprocess_text(i[0]), i[1]) for i in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "b8158bd4-7be2-4aaf-aae5-90905f121570",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_name = \"default_entity\" \n",
    "for i in train_data:\n",
    "    if i[1]['entities'] == []:\n",
    "        i[1]['entities'] = (0, 0, entity_name)\n",
    "    else:\n",
    "        i[1]['entities'][0] = tuple(i[1]['entities'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "16f43476-4f9b-4dc5-8013-235edbc8b9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 353/353 [00:00<00:00, 386.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import spacy.cli\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\") \n",
    "\n",
    "db = DocBin() \n",
    "\n",
    "for text, annot in tqdm(train_data): \n",
    "    doc = nlp.make_doc(text) \n",
    "    ents = []\n",
    "    for start, end, label in annot[\"entities\"]: \n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    doc.ents = ents \n",
    "    db.add(doc)\n",
    "\n",
    "db.to_disk(\"./train.spacy\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cb879d19-88fc-4366-af16-a4ce9dd4a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('annotations_test.json', 'r') as f:\n",
    "    data_test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7fcbc52c-41f9-4e20-9baa-fb284d9b77cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data_test['annotations'] \n",
    "test_data = [(preprocess_text(i[0]), i[1]) for i in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "14a23d55-64f9-4b4e-9fe5-0728ce42e35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_name = \"default_entity\" \n",
    "for i in test_data:\n",
    "    if i[1]['entities'] == []:\n",
    "        i[1]['entities'] = (0, 0, entity_name)\n",
    "    else:\n",
    "        i[1]['entities'][0] = tuple(i[1]['entities'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a061da62-427f-4877-8201-c97536af7241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 162/162 [00:00<00:00, 2586.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import spacy.cli\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\") \n",
    "\n",
    "db = DocBin() \n",
    "\n",
    "for text, annot in tqdm(test_data): \n",
    "    doc = nlp.make_doc(text) \n",
    "    ents = []\n",
    "    for start, end, label in annot[\"entities\"]: \n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    doc.ents = ents \n",
    "    db.add(doc)\n",
    "\n",
    "db.to_disk(\"./test.spacy\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdd4d39-b999-4fda-995a-5d28f23f60c4",
   "metadata": {},
   "source": [
    "Get base confiq file from spacy documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816a1331-be67-4063-8508-404db2da7046",
   "metadata": {},
   "source": [
    "fill the hidden defaults of file by\n",
    "python -m spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bca7ef1-9be8-45ca-87dc-b7e535cc326a",
   "metadata": {},
   "source": [
    "check confiq file and adjust required parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36576514-2b06-4402-8366-a83b323bbf65",
   "metadata": {},
   "source": [
    "train the pipeline by: python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./dev.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01e1f7a3-fc78-47ae-90f9-eed2e4a2af15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tokens: ['pay', '5', 'rupees', 'to', 'vikram', 'kumar', '.']\n",
      "Text: pay 5 rupees to vikram kumar.\n",
      "  5: AMT\n",
      "  vikram kumar: PER\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "\n",
    "model = spacy.load('output/model-best')\n",
    "\n",
    "\n",
    "\n",
    "content =\"pay 5 rupees to vikram kumar.\" \n",
    "\n",
    "sentences = [line.strip() for line in content.strip().split('\\n') if line]\n",
    "\n",
    "\n",
    "for text in sentences:\n",
    "    \n",
    "    doc = model(text)\n",
    "    print(\"Processed tokens:\", [token.text for token in doc])\n",
    "    \n",
    "    print(f\"Text: {text}\")\n",
    "\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        print(f\"  {ent.text}: {ent.label_}\")\n",
    "\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e8f17e0-47a4-451a-9cdf-583dbfb0a3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting build\n",
      "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\pratham\\anaconda3\\lib\\site-packages (from build) (23.1)\n",
      "Collecting pyproject_hooks (from build)\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\pratham\\anaconda3\\lib\\site-packages (from build) (0.4.6)\n",
      "Downloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: pyproject_hooks, build\n",
      "Successfully installed build-1.2.2.post1 pyproject_hooks-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5475ce-34f2-4fe4-8037-69132a7c630d",
   "metadata": {},
   "source": [
    "##########################################################################################\n",
    "\n",
    "After adding normalization to pipeline combine the model and normalization in a single package\n",
    "python -m spacy package modified_model7 output5 --code normalization.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e0134d-30c1-4361-b90d-9b1a03d2bd93",
   "metadata": {},
   "source": [
    "pip install the tar.gz file created inside output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21a43139-c3a1-47ac-85f4-2dcba9d86848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text_refiner', 'tok2vec', 'entity_ruler', 'ner']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_pipeline\")\n",
    "nlp.pipe_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
